# Title card ML detection
Для поиска **короткой заставки** (title card) необходимо выделить `повторяющийся короткий фрагмент`, длительностью примерно от 2 до 10 секунд, который находится в начале серии.

Общий принцип подхода заключается в том, что заставка повторяется из серии в серию. Это означает, что она обладает `похожей визуальной структурой`, включая кадры с названием, стилем и логотипом, а также схожим аудиотреком, таким как фоновая музыка или звуковой сигнал. Следовательно, можно извлечь `видео- и аудиоэмбеддинги из серий`, найти `повторяющиеся фрагменты и сгруппировать их`, выделив заставку как наиболее стабильный и короткий фрагмент.

### Визуальные признаки
Реализация пайплайна включает несколько этапов. На первом этапе `извлекаются визуальные признаки` с использованием модели `ResNet-18`. Для каждой серии извлекаются `кадры с частотой 1 кадр в секунду`. На каждый кадр накладываются предобработки, такие как нормализация и изменение размера. Кадры проходят через ResNet-18, обученную на ImageNet, и из последнего слоя извлекается визуальный `эмбеддинг размерности 512`, что позволяет получить временной ряд визуальных признаков. ResNet выбран за его способность быть достаточно легким и устойчивым к изменению цвета, фона и мелких деталей, при этом улавливая глобальные визуальные паттерны, такие как текст названия или логотип.

### Аудио признаки
На втором этапе `извлекаются аудиопризнаки` с использованием модели `VGGish`. Из серии извлекается аудиодорожка, которая разбивается на короткие фрагменты длительностью 1 секунда. Каждому фрагменту сопоставляется эмбеддинг через VGGish, предобученную модель от Google, обученную на аудио из YouTube.

### Сравнение эмбеддингов
На третьем этапе проводится `сравнение между сериями`. Для серий одного сериала применяется Dynamic Time Warping (DTW) к аудиорядам, чтобы найти `похожие фрагменты`, даже если заставка начинается на разных секундах. Это позволяет учитывать небольшие временные сдвиги. Визуальные эмбеддинги сравниваются по косинусной близости. DTW дает гибкость при сравнении двух серий: если в одной заставка начинается на 0:10, а в другой на 0:07, мы всё равно найдём их как схожие по форме аудиосигнала.

### Кластеризация фрагметов
На четвертом этапе проводится `кластеризация повторяющихся сегментов`. Все визуальные **эмбеддинги фрагментов кластеризуются** с помощью алгоритма `DBSCAN`, который является устойчивым и не требует заранее знать число кластеров. Предполагается, что `наиболее плотный кластер`, встречающийся в нескольких эпизодах, `соответствует заставке`. По индексам кластерных фрагментов восстанавливаются временные интервалы. DBSCAN особенно хорош, если некоторые серии имеют заставку, а другие — нет, так как он просто не выделит кластер для "уникальных" фрагментов.

### Генерация результатов
На последнем этапе формируются предсказания. Для каждого видео из тестовой выборки `выдаётся JSON-файл`, содержащий название файла и временные интервалы начала и конца заставки. Это финальный результат, указывающий, где именно по времени начинается и заканчивается короткая заставка.
